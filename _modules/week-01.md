---
title: Stochastic Model-based Optimization
---
# Abstract
We introduce a new interpretation of first-order methods from the perspective of model function. Using the framework of model-based optimization, the analysis of several existing algorithms can be unified succinctly. Specifically, we focus on the stochastic model-based optimization (SMOD), a class of nonsmooth nonconvex problems which are prevalent in machine learning. To make SMOD more practical, we make two important extensions: 1) we show that the popular momentum trick can be incorporated with SMOD. 2) when the model function satisfies certain good approximation property, even the nonsmooth problems can be linearly accelerated by minibatching. Furthermore, we also briefly cover the topic of distributed optimization and discuss the advantage of certain models in the asynchronous environment.

- Advisor: Qi Deng
- Organizer: Wenzhi Gao


Sep 28
: [Java & Git](#)
  : [1.1](#)

Sep 29
: **Section**{: .label .label-purple }[Intro to Java](#)
  : [Solution](#)

Sep 30
: [Variables & Objects](#)
  : [1.2](#), [2.1](#)

Oct 1
: **Lab**{: .label .label-purple } [Intro to Java](#)

Oct 2
: [Tracing, IntLists, & Recursion](#)
  : [2.1](#)
: **HW 1 due**{: .label .label-red }
